# Backend Support Matrix

Fuse ships with three execution backends. This matrix summarises what works
today so you can pick the right engine (or spot missing features before filing
issues).

| Capability                         | NumPy Runner (`backend="numpy"`)                             | Torch FX Runner (`backend="torch"`)                                                | JAX Runner (`backend="jax"`)                              |
|------------------------------------|--------------------------------------------------------------|-------------------------------------------------------------------------------------|-----------------------------------------------------------|
| Core execution                     | ‚úÖ Full coverage (evaluator implements all builtins)         | üß© FX graph lowering for dense programs; falls back to NumPy for streaming/logical  | üß© Experimental lowering; relies on JAX availability       |
| Gradient support                   | ‚úÖ Symbolic gradients via `generate_gradient_program`        | ‚ö†Ô∏è Autograd integration limited; FX trace exposes graph for external autodiff      | ‚ö†Ô∏è Limited: JIT works for many ops, but gradients rely on NumPy path |
| Streaming / rolling indices        | ‚úÖ Demand + fixpoint modes                                   | ‚ùå Not yet supported (forced NumPy fallback)                                        | ‚ö†Ô∏è Partially supported (compiled as pure JAX where possible) |
| Boolean Datalog operators          | ‚úÖ Supported                                                  | ‚ö†Ô∏è Falls back to NumPy                                                              | ‚ö†Ô∏è Requires NumPy fallback                                 |
| Monte Carlo projection             | ‚úÖ `ExecutionConfig(projection_strategy="monte_carlo")`      | üß© Falls back to NumPy evaluator                                                     | üß© Falls back to NumPy evaluator                           |
| Dtype support                      | `float32` default, `float16`/`bfloat16` via manual casts     | Input tensors follow Torch dtype; internal ops use `float32` by default             | `float32` default; respects JAX dtype policy               |
| Memory-mapped sources (`.npy/.npz`)| ‚úÖ Respects `RuntimePolicies` (with strict mmap option)      | ‚úÖ Inherits manifest handling from shared policy layer                              | ‚úÖ Same manifest layer                                     |
| Device selection                   | CPU only                                                     | CPU/GPU via Torch‚Äôs device strings (`cpu`,`cuda`,`mps`, ‚Ä¶)                          | CPU/GPU/TPU via `ExecutionConfig(device="...")`            |
| FX / export tooling                | N/A                                                          | ‚úÖ FX graph available; packaging helpers in `fuse.interop`                          | ‚ö†Ô∏è Experimental ONNX export via NumPy fallback             |
| Known constraints                  | ‚Äî                                                            | *Demand mode & Monte Carlo fall back to NumPy*<br>*Streaming unsupported*           | *Demand mode & Monte Carlo fall back to NumPy*<br>*Boolean logic falls back*  |

Legend:

* ‚úÖ ‚Äî implemented and used in the test suite
* üß© ‚Äî partially implemented; expect sharp edges or fallbacks
* ‚ö†Ô∏è ‚Äî implemented with caveats; consult source/tests
* ‚ùå ‚Äî not supported today

### Notes

* All backends share manifest loading, runtime policies, and the DSL parser.
  Backend-specific code lives under `src/fuse/*_backend/`.
* Torch and JAX backends both default to 32-bit floats. When compiling with
  mixed precision, ensure the weight manifests supply appropriately typed arrays.
* The Torch backend returns FX `GraphModule`s and caches artifacts on demand;
  reuse the cache directory when packaging models.
* JAX compilation requires `jax[jaxlib] >= 0.4.30`. When JAX is absent, Fuse
  transparently falls back to NumPy execution.

Use this matrix to choose the backend that matches your deployment target, and
to understand which features might still need contribution work.
