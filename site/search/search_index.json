{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Fuse","text":"<p>Fuse is a compact tensor\u2011equation DSL and runtime that maps equations onto NumPy with optional Torch FX and JAX execution paths. It covers sources \u2192 IR \u2192 backends \u2192 sinks in a small, readable codebase.</p> <ul> <li>NumPy evaluator with fixpoint support and rich <code>explain()</code></li> <li>Torch FX lowering and JAX executor with optional XLA callable</li> <li>File sources/sinks, caching, and runtime policies (weights, sharding, quant, LoRA)</li> </ul> <p>Quick links:</p> <ul> <li>Get Started: installation and first run</li> <li>Tutorials: run the <code>.fuse</code> example gallery</li> <li>Concepts: DSL reference and backend matrix</li> <li>API: autogenerated reference from docstrings</li> <li>CLI: <code>python -m fuse run path/to/program.fuse</code></li> </ul>"},{"location":"backend_matrix/","title":"Backend Support Matrix","text":"<p>Fuse ships with three execution backends. This matrix summarises what works today so you can pick the right engine (or spot missing features before filing issues).</p> Capability NumPy Runner (<code>backend=\"numpy\"</code>) Torch FX Runner (<code>backend=\"torch\"</code>) JAX Runner (<code>backend=\"jax\"</code>) Core execution \u2705 Full coverage (evaluator implements all builtins) \ud83e\udde9 FX graph lowering for dense programs; falls back to NumPy for streaming/logical \ud83e\udde9 Experimental lowering; relies on JAX availability Gradient support \u2705 Symbolic gradients via <code>generate_gradient_program</code> \u26a0\ufe0f Autograd integration limited; FX trace exposes graph for external autodiff \u26a0\ufe0f Limited: JIT works for many ops, but gradients rely on NumPy path Streaming / rolling indices \u2705 Demand + fixpoint modes \u274c Not yet supported (forced NumPy fallback) \u26a0\ufe0f Partially supported (compiled as pure JAX where possible) Boolean Datalog operators \u2705 Supported \u26a0\ufe0f Falls back to NumPy \u26a0\ufe0f Requires NumPy fallback Monte Carlo projection \u2705 <code>ExecutionConfig(projection_strategy=\"monte_carlo\")</code> \ud83e\udde9 Falls back to NumPy evaluator \ud83e\udde9 Falls back to NumPy evaluator Dtype support <code>float32</code> default, <code>float16</code>/<code>bfloat16</code> via manual casts Input tensors follow Torch dtype; internal ops use <code>float32</code> by default <code>float32</code> default; respects JAX dtype policy Memory-mapped sources (<code>.npy/.npz</code>) \u2705 Respects <code>RuntimePolicies</code> (with strict mmap option) \u2705 Inherits manifest handling from shared policy layer \u2705 Same manifest layer Device selection CPU only CPU/GPU via Torch\u2019s device strings (<code>cpu</code>,<code>cuda</code>,<code>mps</code>, \u2026) CPU/GPU/TPU via <code>ExecutionConfig(device=\"...\")</code> FX / export tooling N/A \u2705 FX graph available; packaging helpers in <code>fuse.interop</code> \u26a0\ufe0f Experimental ONNX export via NumPy fallback Known constraints \u2014 Demand mode &amp; Monte Carlo fall back to NumPyStreaming unsupported Demand mode &amp; Monte Carlo fall back to NumPyBoolean logic falls back <p>Legend:</p> <ul> <li>\u2705 \u2014 implemented and used in the test suite</li> <li>\ud83e\udde9 \u2014 partially implemented; expect sharp edges or fallbacks</li> <li>\u26a0\ufe0f \u2014 implemented with caveats; consult source/tests</li> <li>\u274c \u2014 not supported today</li> </ul>"},{"location":"backend_matrix/#notes","title":"Notes","text":"<ul> <li>All backends share manifest loading, runtime policies, and the DSL parser.   Backend-specific code lives under <code>src/fuse/*_backend/</code>.</li> <li>Torch and JAX backends both default to 32-bit floats. When compiling with   mixed precision, ensure the weight manifests supply appropriately typed arrays.</li> <li>The Torch backend returns FX <code>GraphModule</code>s and caches artifacts on demand;   reuse the cache directory when packaging models.</li> <li>JAX compilation requires <code>jax[jaxlib] &gt;= 0.4.30</code>. When JAX is absent, Fuse   transparently falls back to NumPy execution.</li> </ul> <p>Use this matrix to choose the backend that matches your deployment target, and to understand which features might still need contribution work.</p>"},{"location":"cli/","title":"Fuse CLI Cheatsheet","text":"<p>Fuse now includes a minimal command-line runner for quick experiments:</p> <pre><code>python -m fuse run program.fuse --backend numpy --out out.npy\n</code></pre>"},{"location":"cli/#usage","title":"Usage","text":"<pre><code>usage: python -m fuse run PROGRAM [--backend numpy|torch|jax] [--out PATH]\n</code></pre> <ul> <li><code>PROGRAM</code> \u2014 path to the <code>.fuse</code> file containing your equations.</li> <li><code>--backend</code> \u2014 execution backend (<code>numpy</code>, <code>torch</code>, or <code>jax</code>). Defaults to   <code>numpy</code>.</li> <li><code>--out</code> \u2014 optional output file. Supports <code>.npy</code>, <code>.npz</code>, <code>.json</code>, and   <code>.jsonl</code>. If omitted, the runner prints the first exported tensor to stdout.</li> </ul> <p>The runner expects the program to declare at least one <code>export</code> statement. When multiple exports are present and <code>--out</code> is omitted, the entire output map is rendered as indented JSON.</p> <p>Backend-specific notes:</p> <ul> <li>Torch and JAX require the respective frameworks installed and fall back to   the NumPy engine when unavailable.</li> <li><code>.npy/.npz</code> outputs honour <code>RuntimePolicies</code> (e.g., memory mapped sources) via   the standard program compilation path.</li> </ul> <p>This utility is intended for CI smoke tests and ad-hoc experimentation; for integrated applications prefer the Python API.</p>"},{"location":"community/","title":"Community","text":""},{"location":"community/#contributing","title":"Contributing","text":"<ul> <li>Use Conventional Commits (e.g., <code>feat: add numpy evaluator guard</code>)</li> <li>Keep PRs focused on one logical change</li> <li>Include reproduction/verification steps and sample outputs when relevant</li> </ul>"},{"location":"community/#development","title":"Development","text":"<pre><code>pip install -e \".[dev]\"\nruff check src tests\nmypy --strict -p fuse\npytest -q\n</code></pre>"},{"location":"community/#docs","title":"Docs","text":"<p>We use MkDocs Material with mkdocstrings. Build locally:</p> <pre><code>pip install -e \".[dev]\"\nmkdocs build --strict\nmkdocs serve\n</code></pre> <p>Versioned deployments can be handled with <code>mike</code>. Configure analytics and versioning in <code>mkdocs.yml</code> or an override file.</p>"},{"location":"dsl_reference/","title":"Fuse DSL Reference","text":"<p>The Fuse front-end is a compact, line-oriented DSL for wiring tensor equations. This page collects the syntax in one place so you can skim the affordances without spelunking through parser code.</p>"},{"location":"dsl_reference/#lexical-structure","title":"Lexical structure","text":"<ul> <li>Programs are plain text. Every non-empty line (ignoring <code>#</code> comments) is a   statement.</li> <li>Statements can span multiple lines by balancing <code>()</code>, <code>[]</code>, or <code>{}</code>. Closing   all delimiters flushes the pending statement.</li> <li>Identifiers begin with <code>A\u2013Z</code>/<code>a\u2013z</code>/<code>_</code> and may contain digits/underscores.   Tensor indices use bare identifiers.</li> <li>String literals use double quotes and are primarily employed for source/sink   file paths.</li> </ul>"},{"location":"dsl_reference/#statements","title":"Statements","text":""},{"location":"dsl_reference/#equations","title":"Equations","text":"<pre><code>Target[i, j] = LHS[i, k] RHS[k, j]\nTarget[i, j] += Bias[i, j]\nTarget[i] max= MaxSources[i, k]\nTarget[i] avg= MeanSources[i, k]\n</code></pre> <p>Supported projection operators on the left hand side:</p> Operator Projection Semantics <code>=</code> <code>sum</code> Default contraction. RHS-only axes are summed out. <code>+=</code> <code>sum</code> Emits a fresh equation sharing the same LHS (sugared add). <code>max=</code> <code>max</code> Projects RHS-only axes by <code>max</code>. <code>avg=</code> <code>mean</code> Projects RHS-only axes by arithmetic mean. <p>LHS indices marked with a trailing <code>.</code> designate dotted axes for reductions. For example <code>Soft[p, q.] = softmax(Logits[p, q])</code> indicates the reduction axis within the builtin call.</p>"},{"location":"dsl_reference/#sources-and-sinks","title":"Sources and sinks","text":"<pre><code>Weights[h, d] = \"ckpt/weights.npy\"    # source\n\"runs/activations.npz\" = Activation[i, j]    # sink\n</code></pre> <p>Sources must use <code>=</code>. Sinks always place the filename on the left and use the default projection (<code>sum</code>).</p>"},{"location":"dsl_reference/#boolean-terms","title":"Boolean terms","text":"<p>Parenthesised names such as <code>Fact(i, j)</code> signal boolean tensors (as opposed to dense numeric tensors <code>Fact[i, j]</code>). Both syntaxes share the same semantics once parsed, but parentheses are a useful convention when mixing boolean logic with dense math.</p>"},{"location":"dsl_reference/#index-functions","title":"Index functions","text":"<p>Fuse includes a small set of unary index functions that emit boolean masks:</p> <pre><code>Even[i]  = even(i)\nOdd[i]   = odd(i)\n</code></pre> <p>These must be supplied an axis either positionally or via <code>axis=</code>. For example <code>even(i)</code> and <code>even(axis=i)</code> are equivalent.</p>"},{"location":"dsl_reference/#function-calls-and-builtins","title":"Function calls and builtins","text":"<p>Common single-argument builtins include:</p> <ul> <li><code>relu</code>, <code>sig</code>, <code>gelu</code>, <code>softmax</code>, <code>lnorm</code>, <code>layernorm</code>, <code>masked_softmax</code>,   <code>attention</code>, <code>rope</code>, <code>concat</code>, <code>causal_mask</code>, <code>topk</code>, <code>const</code>, <code>reduce_max</code>,   <code>reduce_mean</code>, <code>sin</code>, <code>cos</code>, <code>case</code>, <code>tucker_dense</code>.</li> </ul> <p>Arguments can be a tensor expression, a tuple, or include keyword arguments:</p> <pre><code>Soft[p, q.] = softmax(Logits[p, q], axis=\"q\")\nScaled[i, j] = concat(A[i, j], B[i, j], axis=\"j\")\n</code></pre>"},{"location":"dsl_reference/#projection-axis-semantics","title":"Projection &amp; axis semantics","text":"<p>Fuse determines contraction axes by comparing LHS and RHS indices:</p> <ul> <li>Axes that appear on the RHS but not on the LHS are projected.</li> <li>Projection behaviour depends on the operator (<code>sum</code>, <code>max</code>, <code>mean</code> from above).</li> <li>Index order matters. The LHS establishes the storage order for emitted tensors.</li> <li>Shorthand <code>+=</code> splits into separate equations, each sharing the same LHS and   projection descriptor.</li> </ul> <p>Boolean tensors (<code>Fact(i, j)</code>) follow the same projection rules. A projected boolean index (e.g., <code>Fact(i, k)</code> with <code>avg=</code>) will coerce to numeric semantics, so choose the projection that matches your intent.</p>"},{"location":"dsl_reference/#grammar-sketch","title":"Grammar sketch","text":"<pre><code>program        ::= statement*\nstatement      ::= equation | source | sink | export\nequation       ::= lhs operator rhs\nlhs            ::= IDENTIFIER index_spec?\nindex_spec     ::= '[' indices ']' | '(' indices ')'\nindices        ::= IDENTIFIER (',' IDENTIFIER)* ('.')?\noperator       ::= '=' | '+=' | 'max=' | 'avg='\nrhs            ::= sum_term ('+' sum_term)*\nsum_term       ::= product_term (product_term)*\nproduct_term   ::= IDENTIFIER index_spec?\n                 | literal\n                 | function_call\n                 | index_function\nfunction_call  ::= IDENTIFIER '(' arguments? ')'\narguments      ::= expr (',' expr)*\nindex_function ::= IDENTIFIER '(' (IDENTIFIER | 'axis=' IDENTIFIER) ')'\nsource         ::= lhs '=' STRING\nsink           ::= STRING '=' rhs\nexport         ::= 'export' IDENTIFIER\n</code></pre> <p>The grammar above is intentionally approximate: it omits precedence details and the parser accepts extra whitespace and comments, but it captures the core shape of the DSL.</p>"},{"location":"dsl_reference/#quick-checklist","title":"Quick checklist","text":"<ul> <li>Use dotted axes on the LHS to indicate softmax/normalization axes.</li> <li>Remember that <code>+=</code> emits an independent equation; it does not mutate the   previous LHS in place.</li> <li>Index functions (<code>even</code>, <code>odd</code>) require an explicit axis.</li> <li>File sources/sinks always work with <code>np.load</code>/<code>np.save</code> semantics (<code>.npy</code>,   <code>.npz</code>, <code>.jsonl</code>, etc.), respecting runtime policies like memory mapping.</li> </ul> <p>Keep this reference handy while authoring <code>.fuse</code> programs or embedding equations inside Python helpers.</p>"},{"location":"get-started/","title":"Get Started","text":""},{"location":"get-started/#install","title":"Install","text":"<ul> <li>Python 3.9+</li> <li>Optional: Torch (<code>pip install fuse-ai[torch]</code>) and/or JAX (<code>pip install fuse-ai[jax]</code>)</li> </ul> <p>Using a virtualenv:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -e \".[dev]\"\n</code></pre> <p>Or from PyPI:</p> <pre><code>pip install fuse-ai\n# Optional backends\npip install fuse-ai[torch]\npip install fuse-ai[jax]\n</code></pre>"},{"location":"get-started/#first-run","title":"First run","text":"<ul> <li>Run an example:</li> </ul> <pre><code>python examples/01_attention_block.py\n</code></pre> <ul> <li>Or execute a <code>.fuse</code> program via the CLI:</li> </ul> <pre><code>python -m fuse run examples/05_transformer_block.fuse --backend numpy\n</code></pre>"},{"location":"get-started/#next-steps","title":"Next steps","text":"<ul> <li>Browse the Tutorials to run the full example gallery</li> <li>See How\u2011to guides for exporting (TorchScript/ONNX), caching, and policies</li> <li>Explore Concepts for the DSL reference and backend capabilities</li> </ul>"},{"location":"concepts/","title":"Concepts","text":"<p>Key building blocks when working with Fuse:</p> <ul> <li>Program and IR: parse equations and build an intermediate representation</li> <li>ExecutionConfig: control precision, device, fixpoint mode, and projection</li> <li>Runners: NumPy evaluator, Torch FX lowering, JAX executor</li> <li>Policies: weight stores, sharding, quantization, and LoRA adapters</li> </ul> <p>See the DSL Reference for grammar and operators, and the Backend Matrix for capabilities.</p>"},{"location":"how-to/","title":"How\u2011to Guides","text":"<ul> <li>Compile across backends (NumPy/Torch/JAX)</li> <li>Export to TorchScript / ONNX</li> <li>Use caching and artifacts</li> <li>Configure runtime policies (weights, sharding, quant, LoRA)</li> </ul>"},{"location":"how-to/#compile-across-backends","title":"Compile across backends","text":"<pre><code>from fuse import Program\nfrom fuse import torch as fuse_torch\n\nprog = Program(open(\"examples/04_mlp.fuse\").read())\n\n# NumPy\nrunner = prog.compile(backend=\"numpy\")\nrunner()\n\n# Torch FX\nt_runner = fuse_torch.compile(prog, device=\"auto\")\nt_runner()\n</code></pre>"},{"location":"how-to/#export-to-torchscript-onnx","title":"Export to TorchScript / ONNX","text":"<pre><code>from fuse import to_torchscript, to_onnx\n\nts = to_torchscript(prog)  # torch.jit.ScriptModule\nonnx = to_onnx(prog)       # writes a basic ONNX graph\n</code></pre>"},{"location":"how-to/#caching","title":"Caching","text":"<pre><code>runner = prog.compile(backend=\"numpy\", cache_dir=\".cache\")\nrunner()\n</code></pre>"},{"location":"how-to/#runtime-policies-weights-quant-lora","title":"Runtime policies (weights, quant, LoRA)","text":"<pre><code>from fuse import RuntimePolicies, ManifestWeightStore\n\npol = RuntimePolicies(weight_store=ManifestWeightStore(\"examples/ckpt/manifest.json\"))\nrunner = prog.compile(backend=\"numpy\", policies=pol)\n</code></pre>"},{"location":"reference/","title":"API Reference","text":"<p>Autogenerated from Python docstrings via mkdocstrings. Browse the modules using the navigation.</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>fuse</li> <li>__main__</li> <li>core<ul> <li>builtins</li> <li>cache</li> <li>evaluator_numpy</li> <li>exceptions</li> <li>ir</li> <li>parser</li> <li>policies</li> <li>program</li> <li>shape_checker</li> <li>stats</li> <li>temperature</li> </ul> </li> <li>inference<ul> <li>grad_builder</li> <li>tree_program</li> </ul> </li> <li>interop</li> <li>jax_backend<ul> <li>compile</li> </ul> </li> <li>logic</li> <li>nn</li> <li>package</li> <li>pgm</li> <li>torch_backend<ul> <li>compile</li> </ul> </li> <li>training<ul> <li>bpts</li> </ul> </li> </ul>"},{"location":"reference/fuse/","title":"fuse","text":""},{"location":"reference/fuse/#fuse","title":"<code>fuse</code>","text":""},{"location":"reference/fuse/#fuse.ExecutionConfig","title":"<code>ExecutionConfig</code>  <code>dataclass</code>","text":"<p>Common execution switches shared across the NumPy/Torch/JAX runtimes.</p> <p>Key behaviors: * <code>precision</code> defaults to <code>\"fp32\"</code> and can be set to <code>\"bf16\"</code>, <code>\"fp16\"</code>,   or <code>\"auto\"</code> for backends that support mixed-precision lowering. * <code>device</code> tracks the desired target (<code>\"cpu\"</code>, <code>\"cuda\"</code>, <code>\"mps\"</code> or <code>\"auto\"</code>)   so Torch FX and JAX lowerings can keep device placement aligned with NumPy fallbacks. * <code>zero_copy</code> enables zero-copy host handoffs whenever possible to avoid   unnecessary host \u2194 device transfers in hybrid execution paths.</p> Source code in <code>src/fuse/core/evaluator_numpy.py</code> <pre><code>@dataclass(frozen=True)\nclass ExecutionConfig:\n    \"\"\"\n    Common execution switches shared across the NumPy/Torch/JAX runtimes.\n\n    Key behaviors:\n    * ``precision`` defaults to ``\"fp32\"`` and can be set to ``\"bf16\"``, ``\"fp16\"``,\n      or ``\"auto\"`` for backends that support mixed-precision lowering.\n    * ``device`` tracks the desired target (``\"cpu\"``, ``\"cuda\"``, ``\"mps\"`` or ``\"auto\"``)\n      so Torch FX and JAX lowerings can keep device placement aligned with NumPy fallbacks.\n    * ``zero_copy`` enables zero-copy host handoffs whenever possible to avoid\n      unnecessary host \u2194 device transfers in hybrid execution paths.\n    \"\"\"\n\n    mode: str = \"single\"  # \"single\" | \"fixpoint\" | \"demand\"\n    fixpoint_strategy: str = \"synchronous\"  # \"synchronous\" | \"semi_naive\"\n    max_iters: int = 32\n    tol: float = 1e-6\n    chaining: str = \"forward\"  # \"forward\" | \"backward\"\n    explain_timings: bool = True\n    projection_strategy: str = \"exact\"  # \"exact\" | \"monte_carlo\"\n    projection_samples: Optional[int] = None\n    projection_seed: Optional[int] = None\n    temperatures: Optional[Dict[str, TemperatureSchedule]] = None\n    precision: str = \"fp32\"  # \"fp32\" | \"bf16\" | \"fp16\" | \"auto\"\n    device: str = \"auto\"  # \"auto\" | \"cpu\" | \"cuda\" | \"mps\" | \"cuda:N\"\n    zero_copy: bool = True\n    jax_enable_xla_cache: bool = False\n    jax_cache_dir: Optional[str] = None\n    validate_device_transfers: bool = False\n    block_size: Optional[int] = None\n\n    def normalized(self) -&gt; \"ExecutionConfig\":\n        mode = self.mode.lower()\n        if mode not in {\"single\", \"fixpoint\", \"demand\"}:\n            raise ValueError(f\"Unsupported execution mode: {self.mode}\")\n        fixpoint = (self.fixpoint_strategy or \"synchronous\").lower()\n        if fixpoint not in {\"synchronous\", \"semi_naive\"}:\n            raise ValueError(f\"Unsupported fixpoint strategy: {self.fixpoint_strategy}\")\n        chaining = self.chaining.lower()\n        if chaining not in {\"forward\", \"backward\"}:\n            raise ValueError(f\"Unsupported chaining mode: {self.chaining}\")\n        strategy = self.projection_strategy.lower()\n        if strategy not in {\"exact\", \"monte_carlo\"}:\n            raise ValueError(f\"Unsupported projection strategy: {self.projection_strategy}\")\n        samples = self.projection_samples\n        if samples is not None:\n            samples = int(samples)\n            if samples &lt;= 0:\n                raise ValueError(\"projection_samples must be positive when provided\")\n        seed = self.projection_seed\n        if seed is not None:\n            seed = int(seed)\n        temperatures = normalize_temperature_map(self.temperatures)\n        precision = (self.precision or \"fp32\").lower()\n        if precision not in {\"fp32\", \"bf16\", \"fp16\", \"auto\"}:\n            raise ValueError(f\"Unsupported precision setting: {self.precision}\")\n        device = _normalize_device_spec(self.device)\n        zero_copy = bool(self.zero_copy)\n        jax_enable_cache = bool(self.jax_enable_xla_cache)\n        cache_dir = self.jax_cache_dir\n        if cache_dir is not None:\n            cache_dir = str(Path(cache_dir).expanduser())\n        validate_transfers = bool(self.validate_device_transfers)\n        block_size = self.block_size\n        if block_size is not None:\n            block_size = int(block_size)\n            if block_size &lt;= 0:\n                raise ValueError(\"block_size must be positive when provided\")\n        return replace(\n            self,\n            mode=mode,\n            fixpoint_strategy=fixpoint,\n            chaining=chaining,\n            projection_strategy=strategy,\n            projection_samples=samples,\n            projection_seed=seed,\n            temperatures=temperatures,\n            precision=precision,\n            device=device,\n            zero_copy=zero_copy,\n            jax_enable_xla_cache=jax_enable_cache,\n            jax_cache_dir=cache_dir,\n            validate_device_transfers=validate_transfers,\n            block_size=block_size,\n        )\n</code></pre>"},{"location":"reference/fuse/#fuse.ShardingPolicy","title":"<code>ShardingPolicy</code>  <code>dataclass</code>","text":"Source code in <code>src/fuse/core/policies.py</code> <pre><code>@dataclass\nclass ShardingPolicy:\n    strategy: str = \"replicated\"  # e.g., replicated, row, column\n    mesh: Optional[Any] = None\n    attributes: Dict[str, Any] = field(default_factory=dict)\n\n    def materialize(self, name: str, value: Any) -&gt; Any:\n        \"\"\"Apply sharding metadata; numpy backend treats everything as local.\"\"\"\n        return value\n\n    def fingerprint(self) -&gt; Dict[str, Any]:\n        return {\n            \"strategy\": self.strategy,\n            \"mesh\": _sanitize_metadata(self.mesh),\n            \"attributes\": _sanitize_metadata(self.attributes),\n        }\n</code></pre>"},{"location":"reference/fuse/#fuse.ShardingPolicy.materialize","title":"<code>materialize(name, value)</code>","text":"<p>Apply sharding metadata; numpy backend treats everything as local.</p> Source code in <code>src/fuse/core/policies.py</code> <pre><code>def materialize(self, name: str, value: Any) -&gt; Any:\n    \"\"\"Apply sharding metadata; numpy backend treats everything as local.\"\"\"\n    return value\n</code></pre>"},{"location":"reference/fuse/#fuse.from_pytorch","title":"<code>from_pytorch(state_dict, mapping, *, strict=True)</code>","text":"<p>Convert a PyTorch state dict into Fuse weight tensors with named-axis remapping.</p>"},{"location":"reference/fuse/#fuse.from_pytorch--parameters","title":"Parameters","text":"<p>state_dict:     Mapping of parameter names to tensors (typically <code>torch.Tensor</code>). mapping:     Mapping from desired Fuse tensor names to a specification dictionary.     Each specification supports:</p> <pre><code>- ``key`` (str): source key in the state dict (required)\n- ``source_axes`` (Sequence[str]): axis names describing the source order\n- ``target_axes`` (Sequence[str]): desired axis order for Fuse tensors\n- ``transpose`` (bool): optional convenience flag for 2-D transpose\n- ``dtype`` (np.dtype or str): optional dtype cast\n</code></pre> <p>strict:     If True, missing keys raise an error. Otherwise they are skipped.</p>"},{"location":"reference/fuse/#fuse.from_pytorch--returns","title":"Returns","text":"<p>Dict[str, np.ndarray]     Mapping of Fuse tensor names to NumPy arrays in the requested axis order.</p> Source code in <code>src/fuse/interop.py</code> <pre><code>def from_pytorch(\n    state_dict: Mapping[str, Any],\n    mapping: Mapping[str, Mapping[str, Any]],\n    *,\n    strict: bool = True,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Convert a PyTorch state dict into Fuse weight tensors with named-axis remapping.\n\n    Parameters\n    ----------\n    state_dict:\n        Mapping of parameter names to tensors (typically ``torch.Tensor``).\n    mapping:\n        Mapping from desired Fuse tensor names to a specification dictionary.\n        Each specification supports:\n\n        - ``key`` (str): source key in the state dict (required)\n        - ``source_axes`` (Sequence[str]): axis names describing the source order\n        - ``target_axes`` (Sequence[str]): desired axis order for Fuse tensors\n        - ``transpose`` (bool): optional convenience flag for 2-D transpose\n        - ``dtype`` (np.dtype or str): optional dtype cast\n    strict:\n        If True, missing keys raise an error. Otherwise they are skipped.\n\n    Returns\n    -------\n    Dict[str, np.ndarray]\n        Mapping of Fuse tensor names to NumPy arrays in the requested axis order.\n    \"\"\"\n\n    weights: Dict[str, np.ndarray] = {}\n    for target_name, spec in mapping.items():\n        arr = _load_spec_tensor(state_dict, spec, strict=strict)\n        if arr is None:\n            continue\n        weights[target_name] = arr\n    return weights\n</code></pre>"},{"location":"reference/fuse/#fuse.from_safetensors","title":"<code>from_safetensors(path, mapping, *, strict=True)</code>","text":"<p>Load weights from a <code>.safetensors</code> file with named-axis remapping.</p> Source code in <code>src/fuse/interop.py</code> <pre><code>def from_safetensors(\n    path: Union[str, Path],\n    mapping: Mapping[str, Mapping[str, Any]],\n    *,\n    strict: bool = True,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Load weights from a ``.safetensors`` file with named-axis remapping.\n    \"\"\"\n    try:\n        from safetensors.numpy import load_file as load_safetensors  # type: ignore\n    except Exception as exc:  # pragma: no cover - optional dependency\n        raise RuntimeError(\"safetensors is required for from_safetensors\") from exc\n\n    data = load_safetensors(str(path))\n    return from_pytorch(data, mapping, strict=strict)\n</code></pre>"},{"location":"reference/fuse/#fuse.to_onnx","title":"<code>to_onnx(program, example_inputs, *, policies=None, device='auto', config=None, file_path=None, opset_version=17, dynamic_axes=None)</code>","text":"<p>Export a Fuse program to ONNX using PyTorch tracing.</p> Source code in <code>src/fuse/interop.py</code> <pre><code>def to_onnx(\n    program: Program,\n    example_inputs: Mapping[str, Any],\n    *,\n    policies: Optional[RuntimePolicies] = None,\n    device: str = \"auto\",\n    config: Optional[Any] = None,\n    file_path: Optional[Union[str, Path]] = None,\n    opset_version: int = 17,\n    dynamic_axes: Optional[Dict[str, Dict[int, str]]] = None,\n) -&gt; Union[bytes, Path]:\n    \"\"\"\n    Export a Fuse program to ONNX using PyTorch tracing.\n    \"\"\"\n    _ensure_torch_available()\n    runtime_policies = _ensure_runtime_policies(policies)\n    runner = program.compile(\n        backend=\"torch\",\n        device=device,\n        config=config,\n        policies=runtime_policies,\n    )\n    input_names, tensors = _prepare_example_tensors(example_inputs, runner.device)\n    module = _FuseTorchModule(runner, input_names)\n    module.eval()\n\n    output_names = list(program.ir.exports)\n\n    kwargs = {\n        \"input_names\": input_names,\n        \"output_names\": output_names,\n        \"opset_version\": opset_version,\n        \"dynamic_axes\": dynamic_axes or {},\n    }\n\n    if file_path is None:\n        buffer = io.BytesIO()\n        torch.onnx.export(\n            module,\n            tuple(tensors),\n            buffer,\n            **kwargs,\n        )\n        buffer.seek(0)\n        return buffer.getvalue()\n\n    torch.onnx.export(\n        module,\n        tuple(tensors),\n        str(file_path),\n        **kwargs,\n    )\n    return Path(file_path)\n</code></pre>"},{"location":"reference/fuse/#fuse.to_torchscript","title":"<code>to_torchscript(program, example_inputs, *, policies=None, device='auto', config=None, file_path=None)</code>","text":"<p>Trace a Fuse program into a TorchScript module using example inputs.</p> Source code in <code>src/fuse/interop.py</code> <pre><code>def to_torchscript(\n    program: Program,\n    example_inputs: Mapping[str, Any],\n    *,\n    policies: Optional[RuntimePolicies] = None,\n    device: str = \"auto\",\n    config: Optional[Any] = None,\n    file_path: Optional[Union[str, Path]] = None,\n) -&gt; \"torch.jit.ScriptModule\":\n    \"\"\"\n    Trace a Fuse program into a TorchScript module using example inputs.\n    \"\"\"\n    _ensure_torch_available()\n    runtime_policies = _ensure_runtime_policies(policies)\n    runner = program.compile(\n        backend=\"torch\",\n        device=device,\n        config=config,\n        policies=runtime_policies,\n    )\n    input_names, tensors = _prepare_example_tensors(example_inputs, runner.device)\n    module = _FuseTorchModule(runner, input_names)\n    module.eval()\n    with torch.no_grad():\n        scripted = torch.jit.trace(module, tuple(tensors), strict=False)\n    if file_path is not None:\n        scripted.save(str(file_path))\n    return scripted\n</code></pre>"},{"location":"reference/fuse/#fuse.gradients_for_program","title":"<code>gradients_for_program(program, *, seeds, grad_tensors, backend='numpy', config=None, policies=None)</code>","text":"<p>Evaluate per-example gradients for the provided program by generating a symbolic gradient program (BPTS-style).</p>"},{"location":"reference/fuse/#fuse.gradients_for_program--parameters","title":"Parameters","text":"<p>program:     Compiled representation of the example-specific Fuse program. seeds:     Mapping of tensor name to Fuse expression used to seed its gradient     (e.g., <code>{\"Loss\": \"const(1.0)\"}</code>). grad_tensors:     Iterable of tensor names whose gradients should be returned. backend:     Backend used to execute the gradient program (defaults to \"numpy\"). config:     Optional execution config; defaults to <code>ExecutionConfig(mode=\"single\")</code>. policies:     Optional runtime policies passed to the compiled gradient program.</p>"},{"location":"reference/fuse/#fuse.gradients_for_program--returns","title":"Returns","text":"<p>grads:     Dictionary mapping tensor names to <code>np.ndarray</code> gradients. raw_outputs:     Complete output dictionary from the gradient program execution.</p> Source code in <code>src/fuse/training/bpts.py</code> <pre><code>def gradients_for_program(\n    program: Program,\n    *,\n    seeds: Dict[str, str],\n    grad_tensors: Sequence[str],\n    backend: str = \"numpy\",\n    config: Optional[ExecutionConfig] = None,\n    policies: Optional[RuntimePolicies] = None,\n) -&gt; Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n    \"\"\"\n    Evaluate per-example gradients for the provided program by generating\n    a symbolic gradient program (BPTS-style).\n\n    Parameters\n    ----------\n    program:\n        Compiled representation of the example-specific Fuse program.\n    seeds:\n        Mapping of tensor name to Fuse expression used to seed its gradient\n        (e.g., ``{\\\"Loss\\\": \\\"const(1.0)\\\"}``).\n    grad_tensors:\n        Iterable of tensor names whose gradients should be returned.\n    backend:\n        Backend used to execute the gradient program (defaults to \"numpy\").\n    config:\n        Optional execution config; defaults to ``ExecutionConfig(mode=\\\"single\\\")``.\n    policies:\n        Optional runtime policies passed to the compiled gradient program.\n\n    Returns\n    -------\n    grads:\n        Dictionary mapping tensor names to ``np.ndarray`` gradients.\n    raw_outputs:\n        Complete output dictionary from the gradient program execution.\n    \"\"\"\n    grad_program = generate_gradient_program(\n        program,\n        seeds=seeds,\n        export_grads=grad_tensors,\n    )\n    runner = grad_program.program.compile(\n        backend=backend,\n        config=config or ExecutionConfig(mode=\"single\"),\n        policies=policies,\n    )\n    outputs = runner()\n    grads: Dict[str, np.ndarray] = {}\n    for tensor in grad_tensors:\n        name = _grad_name(tensor)\n        grad_value = outputs.get(name)\n        if grad_value is None:\n            raise KeyError(f\"Gradient output '{name}' missing from gradient program\")\n        grads[tensor] = np.asarray(grad_value)\n    return grads, outputs\n</code></pre>"},{"location":"reference/fuse/__main__/","title":"fuse.main","text":""},{"location":"reference/fuse/__main__/#fuse.__main__","title":"<code>fuse.__main__</code>","text":""},{"location":"reference/fuse/core/","title":"fuse.core","text":""},{"location":"reference/fuse/core/#fuse.core","title":"<code>fuse.core</code>","text":"<p>Core runtime modules for Fuse.</p>"},{"location":"reference/fuse/core/builtins/","title":"fuse.core.builtins","text":""},{"location":"reference/fuse/core/builtins/#fuse.core.builtins","title":"<code>fuse.core.builtins</code>","text":""},{"location":"reference/fuse/core/builtins/#fuse.core.builtins.BagOfWordsTensor","title":"<code>BagOfWordsTensor</code>","text":"<p>Dense bag-of-words matrix that preserves the vocabulary mapping.</p> <p>Behaves like a NumPy array for downstream consumers while exposing <code>vocab</code> so callers can recover the token ordering.</p> Source code in <code>src/fuse/core/builtins.py</code> <pre><code>class BagOfWordsTensor:\n    \"\"\"\n    Dense bag-of-words matrix that preserves the vocabulary mapping.\n\n    Behaves like a NumPy array for downstream consumers while exposing\n    ``vocab`` so callers can recover the token ordering.\n    \"\"\"\n\n    def __init__(\n        self, matrix: np.ndarray, vocab: Dict[str, int], vocab_path: Optional[Path] = None\n    ):\n        self.matrix = np.asarray(matrix, dtype=np.int8)\n        self.vocab = dict(vocab)\n        self.vocab_path = vocab_path\n\n    def __array__(self, dtype=None):\n        if dtype is not None:\n            return self.matrix.astype(dtype)\n        return self.matrix\n\n    def to_dense(self) -&gt; np.ndarray:\n        return np.asarray(self)\n\n    def __repr__(self) -&gt; str:  # pragma: no cover - repr only used for debugging\n        return f\"BagOfWordsTensor(shape={self.matrix.shape}, vocab_size={len(self.vocab)})\"\n</code></pre>"},{"location":"reference/fuse/core/builtins/#fuse.core.builtins.tucker_dense","title":"<code>tucker_dense(value, rank=None, threshold=0.5, *, rng=None)</code>","text":"<p>Approximate a sparse/high-order relation with a low-rank Tucker reconstruction and return a denoised dense tensor via step().</p> Source code in <code>src/fuse/core/builtins.py</code> <pre><code>def tucker_dense(\n    value, rank=None, threshold: float = 0.5, *, rng: Optional[np.random.Generator] = None\n):\n    \"\"\"\n    Approximate a sparse/high-order relation with a low-rank Tucker reconstruction\n    and return a denoised dense tensor via step().\n    \"\"\"\n    arr = _ensure_float_array(value)\n    if arr.ndim == 0:\n        return step(arr - threshold)\n    ranks = _normalize_tucker_ranks(rank, arr.shape)\n    if all(r == dim for r, dim in zip(ranks, arr.shape)):\n        approx = arr\n    else:\n        core, factors = _tucker_decompose_np(arr, ranks, rng=rng)\n        approx = _tucker_reconstruct_np(core, factors)\n    approx = approx - float(threshold)\n    return step(approx)\n</code></pre>"},{"location":"reference/fuse/core/cache/","title":"fuse.core.cache","text":""},{"location":"reference/fuse/core/cache/#fuse.core.cache","title":"<code>fuse.core.cache</code>","text":""},{"location":"reference/fuse/core/evaluator_numpy/","title":"fuse.core.evaluator_numpy","text":""},{"location":"reference/fuse/core/evaluator_numpy/#fuse.core.evaluator_numpy","title":"<code>fuse.core.evaluator_numpy</code>","text":""},{"location":"reference/fuse/core/evaluator_numpy/#fuse.core.evaluator_numpy.ExecutionConfig","title":"<code>ExecutionConfig</code>  <code>dataclass</code>","text":"<p>Common execution switches shared across the NumPy/Torch/JAX runtimes.</p> <p>Key behaviors: * <code>precision</code> defaults to <code>\"fp32\"</code> and can be set to <code>\"bf16\"</code>, <code>\"fp16\"</code>,   or <code>\"auto\"</code> for backends that support mixed-precision lowering. * <code>device</code> tracks the desired target (<code>\"cpu\"</code>, <code>\"cuda\"</code>, <code>\"mps\"</code> or <code>\"auto\"</code>)   so Torch FX and JAX lowerings can keep device placement aligned with NumPy fallbacks. * <code>zero_copy</code> enables zero-copy host handoffs whenever possible to avoid   unnecessary host \u2194 device transfers in hybrid execution paths.</p> Source code in <code>src/fuse/core/evaluator_numpy.py</code> <pre><code>@dataclass(frozen=True)\nclass ExecutionConfig:\n    \"\"\"\n    Common execution switches shared across the NumPy/Torch/JAX runtimes.\n\n    Key behaviors:\n    * ``precision`` defaults to ``\"fp32\"`` and can be set to ``\"bf16\"``, ``\"fp16\"``,\n      or ``\"auto\"`` for backends that support mixed-precision lowering.\n    * ``device`` tracks the desired target (``\"cpu\"``, ``\"cuda\"``, ``\"mps\"`` or ``\"auto\"``)\n      so Torch FX and JAX lowerings can keep device placement aligned with NumPy fallbacks.\n    * ``zero_copy`` enables zero-copy host handoffs whenever possible to avoid\n      unnecessary host \u2194 device transfers in hybrid execution paths.\n    \"\"\"\n\n    mode: str = \"single\"  # \"single\" | \"fixpoint\" | \"demand\"\n    fixpoint_strategy: str = \"synchronous\"  # \"synchronous\" | \"semi_naive\"\n    max_iters: int = 32\n    tol: float = 1e-6\n    chaining: str = \"forward\"  # \"forward\" | \"backward\"\n    explain_timings: bool = True\n    projection_strategy: str = \"exact\"  # \"exact\" | \"monte_carlo\"\n    projection_samples: Optional[int] = None\n    projection_seed: Optional[int] = None\n    temperatures: Optional[Dict[str, TemperatureSchedule]] = None\n    precision: str = \"fp32\"  # \"fp32\" | \"bf16\" | \"fp16\" | \"auto\"\n    device: str = \"auto\"  # \"auto\" | \"cpu\" | \"cuda\" | \"mps\" | \"cuda:N\"\n    zero_copy: bool = True\n    jax_enable_xla_cache: bool = False\n    jax_cache_dir: Optional[str] = None\n    validate_device_transfers: bool = False\n    block_size: Optional[int] = None\n\n    def normalized(self) -&gt; \"ExecutionConfig\":\n        mode = self.mode.lower()\n        if mode not in {\"single\", \"fixpoint\", \"demand\"}:\n            raise ValueError(f\"Unsupported execution mode: {self.mode}\")\n        fixpoint = (self.fixpoint_strategy or \"synchronous\").lower()\n        if fixpoint not in {\"synchronous\", \"semi_naive\"}:\n            raise ValueError(f\"Unsupported fixpoint strategy: {self.fixpoint_strategy}\")\n        chaining = self.chaining.lower()\n        if chaining not in {\"forward\", \"backward\"}:\n            raise ValueError(f\"Unsupported chaining mode: {self.chaining}\")\n        strategy = self.projection_strategy.lower()\n        if strategy not in {\"exact\", \"monte_carlo\"}:\n            raise ValueError(f\"Unsupported projection strategy: {self.projection_strategy}\")\n        samples = self.projection_samples\n        if samples is not None:\n            samples = int(samples)\n            if samples &lt;= 0:\n                raise ValueError(\"projection_samples must be positive when provided\")\n        seed = self.projection_seed\n        if seed is not None:\n            seed = int(seed)\n        temperatures = normalize_temperature_map(self.temperatures)\n        precision = (self.precision or \"fp32\").lower()\n        if precision not in {\"fp32\", \"bf16\", \"fp16\", \"auto\"}:\n            raise ValueError(f\"Unsupported precision setting: {self.precision}\")\n        device = _normalize_device_spec(self.device)\n        zero_copy = bool(self.zero_copy)\n        jax_enable_cache = bool(self.jax_enable_xla_cache)\n        cache_dir = self.jax_cache_dir\n        if cache_dir is not None:\n            cache_dir = str(Path(cache_dir).expanduser())\n        validate_transfers = bool(self.validate_device_transfers)\n        block_size = self.block_size\n        if block_size is not None:\n            block_size = int(block_size)\n            if block_size &lt;= 0:\n                raise ValueError(\"block_size must be positive when provided\")\n        return replace(\n            self,\n            mode=mode,\n            fixpoint_strategy=fixpoint,\n            chaining=chaining,\n            projection_strategy=strategy,\n            projection_samples=samples,\n            projection_seed=seed,\n            temperatures=temperatures,\n            precision=precision,\n            device=device,\n            zero_copy=zero_copy,\n            jax_enable_xla_cache=jax_enable_cache,\n            jax_cache_dir=cache_dir,\n            validate_device_transfers=validate_transfers,\n            block_size=block_size,\n        )\n</code></pre>"},{"location":"reference/fuse/core/exceptions/","title":"fuse.core.exceptions","text":""},{"location":"reference/fuse/core/exceptions/#fuse.core.exceptions","title":"<code>fuse.core.exceptions</code>","text":""},{"location":"reference/fuse/core/exceptions/#fuse.core.exceptions.FuseError","title":"<code>FuseError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base class for Fuse-specific exceptions.</p> Source code in <code>src/fuse/core/exceptions.py</code> <pre><code>class FuseError(Exception):\n    \"\"\"Base class for Fuse-specific exceptions.\"\"\"\n</code></pre>"},{"location":"reference/fuse/core/ir/","title":"fuse.core.ir","text":""},{"location":"reference/fuse/core/ir/#fuse.core.ir","title":"<code>fuse.core.ir</code>","text":""},{"location":"reference/fuse/core/parser/","title":"fuse.core.parser","text":""},{"location":"reference/fuse/core/parser/#fuse.core.parser","title":"<code>fuse.core.parser</code>","text":""},{"location":"reference/fuse/core/parser/#fuse.core.parser.SumList","title":"<code>SumList</code>","text":"<p>               Bases: <code>list</code></p> <p>Marker list for additive expression groups.</p> Source code in <code>src/fuse/core/parser.py</code> <pre><code>class SumList(list):\n    \"\"\"Marker list for additive expression groups.\"\"\"\n</code></pre>"},{"location":"reference/fuse/core/policies/","title":"fuse.core.policies","text":""},{"location":"reference/fuse/core/policies/#fuse.core.policies","title":"<code>fuse.core.policies</code>","text":""},{"location":"reference/fuse/core/policies/#fuse.core.policies.WeightStore","title":"<code>WeightStore</code>","text":"<p>               Bases: <code>Protocol</code></p> Source code in <code>src/fuse/core/policies.py</code> <pre><code>class WeightStore(Protocol):\n    def resolve(self, name: str) -&gt; Any:\n        \"\"\"Return a payload representing the requested weight.\"\"\"\n</code></pre>"},{"location":"reference/fuse/core/policies/#fuse.core.policies.WeightStore.resolve","title":"<code>resolve(name)</code>","text":"<p>Return a payload representing the requested weight.</p> Source code in <code>src/fuse/core/policies.py</code> <pre><code>def resolve(self, name: str) -&gt; Any:\n    \"\"\"Return a payload representing the requested weight.\"\"\"\n</code></pre>"},{"location":"reference/fuse/core/policies/#fuse.core.policies.ShardingPolicy","title":"<code>ShardingPolicy</code>  <code>dataclass</code>","text":"Source code in <code>src/fuse/core/policies.py</code> <pre><code>@dataclass\nclass ShardingPolicy:\n    strategy: str = \"replicated\"  # e.g., replicated, row, column\n    mesh: Optional[Any] = None\n    attributes: Dict[str, Any] = field(default_factory=dict)\n\n    def materialize(self, name: str, value: Any) -&gt; Any:\n        \"\"\"Apply sharding metadata; numpy backend treats everything as local.\"\"\"\n        return value\n\n    def fingerprint(self) -&gt; Dict[str, Any]:\n        return {\n            \"strategy\": self.strategy,\n            \"mesh\": _sanitize_metadata(self.mesh),\n            \"attributes\": _sanitize_metadata(self.attributes),\n        }\n</code></pre>"},{"location":"reference/fuse/core/policies/#fuse.core.policies.ShardingPolicy.materialize","title":"<code>materialize(name, value)</code>","text":"<p>Apply sharding metadata; numpy backend treats everything as local.</p> Source code in <code>src/fuse/core/policies.py</code> <pre><code>def materialize(self, name: str, value: Any) -&gt; Any:\n    \"\"\"Apply sharding metadata; numpy backend treats everything as local.\"\"\"\n    return value\n</code></pre>"},{"location":"reference/fuse/core/program/","title":"fuse.core.program","text":""},{"location":"reference/fuse/core/program/#fuse.core.program","title":"<code>fuse.core.program</code>","text":""},{"location":"reference/fuse/core/shape_checker/","title":"fuse.core.shape_checker","text":""},{"location":"reference/fuse/core/shape_checker/#fuse.core.shape_checker","title":"<code>fuse.core.shape_checker</code>","text":""},{"location":"reference/fuse/core/stats/","title":"fuse.core.stats","text":""},{"location":"reference/fuse/core/stats/#fuse.core.stats","title":"<code>fuse.core.stats</code>","text":""},{"location":"reference/fuse/core/temperature/","title":"fuse.core.temperature","text":""},{"location":"reference/fuse/core/temperature/#fuse.core.temperature","title":"<code>fuse.core.temperature</code>","text":""},{"location":"reference/fuse/core/temperature/#fuse.core.temperature.TemperatureSchedule","title":"<code>TemperatureSchedule</code>","text":"<p>Callable schedule that produces a temperature for a given iteration.</p> Source code in <code>src/fuse/core/temperature.py</code> <pre><code>class TemperatureSchedule:\n    \"\"\"Callable schedule that produces a temperature for a given iteration.\"\"\"\n\n    def value(self, iteration: int) -&gt; float:\n        raise NotImplementedError\n\n    def manifest(self) -&gt; Dict[str, Any]:\n        return {\"type\": self.__class__.__name__}\n\n    def __call__(self, iteration: int) -&gt; float:\n        return self.value(iteration)\n</code></pre>"},{"location":"reference/fuse/inference/","title":"fuse.inference","text":""},{"location":"reference/fuse/inference/#fuse.inference","title":"<code>fuse.inference</code>","text":"<p>Inference utilities for Fuse programs.</p>"},{"location":"reference/fuse/inference/grad_builder/","title":"fuse.inference.grad_builder","text":""},{"location":"reference/fuse/inference/grad_builder/#fuse.inference.grad_builder","title":"<code>fuse.inference.grad_builder</code>","text":""},{"location":"reference/fuse/inference/tree_program/","title":"fuse.inference.tree_program","text":""},{"location":"reference/fuse/inference/tree_program/#fuse.inference.tree_program","title":"<code>fuse.inference.tree_program</code>","text":""},{"location":"reference/fuse/interop/","title":"fuse.interop","text":""},{"location":"reference/fuse/interop/#fuse.interop","title":"<code>fuse.interop</code>","text":""},{"location":"reference/fuse/interop/#fuse.interop.from_pytorch","title":"<code>from_pytorch(state_dict, mapping, *, strict=True)</code>","text":"<p>Convert a PyTorch state dict into Fuse weight tensors with named-axis remapping.</p>"},{"location":"reference/fuse/interop/#fuse.interop.from_pytorch--parameters","title":"Parameters","text":"<p>state_dict:     Mapping of parameter names to tensors (typically <code>torch.Tensor</code>). mapping:     Mapping from desired Fuse tensor names to a specification dictionary.     Each specification supports:</p> <pre><code>- ``key`` (str): source key in the state dict (required)\n- ``source_axes`` (Sequence[str]): axis names describing the source order\n- ``target_axes`` (Sequence[str]): desired axis order for Fuse tensors\n- ``transpose`` (bool): optional convenience flag for 2-D transpose\n- ``dtype`` (np.dtype or str): optional dtype cast\n</code></pre> <p>strict:     If True, missing keys raise an error. Otherwise they are skipped.</p>"},{"location":"reference/fuse/interop/#fuse.interop.from_pytorch--returns","title":"Returns","text":"<p>Dict[str, np.ndarray]     Mapping of Fuse tensor names to NumPy arrays in the requested axis order.</p> Source code in <code>src/fuse/interop.py</code> <pre><code>def from_pytorch(\n    state_dict: Mapping[str, Any],\n    mapping: Mapping[str, Mapping[str, Any]],\n    *,\n    strict: bool = True,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Convert a PyTorch state dict into Fuse weight tensors with named-axis remapping.\n\n    Parameters\n    ----------\n    state_dict:\n        Mapping of parameter names to tensors (typically ``torch.Tensor``).\n    mapping:\n        Mapping from desired Fuse tensor names to a specification dictionary.\n        Each specification supports:\n\n        - ``key`` (str): source key in the state dict (required)\n        - ``source_axes`` (Sequence[str]): axis names describing the source order\n        - ``target_axes`` (Sequence[str]): desired axis order for Fuse tensors\n        - ``transpose`` (bool): optional convenience flag for 2-D transpose\n        - ``dtype`` (np.dtype or str): optional dtype cast\n    strict:\n        If True, missing keys raise an error. Otherwise they are skipped.\n\n    Returns\n    -------\n    Dict[str, np.ndarray]\n        Mapping of Fuse tensor names to NumPy arrays in the requested axis order.\n    \"\"\"\n\n    weights: Dict[str, np.ndarray] = {}\n    for target_name, spec in mapping.items():\n        arr = _load_spec_tensor(state_dict, spec, strict=strict)\n        if arr is None:\n            continue\n        weights[target_name] = arr\n    return weights\n</code></pre>"},{"location":"reference/fuse/interop/#fuse.interop.from_safetensors","title":"<code>from_safetensors(path, mapping, *, strict=True)</code>","text":"<p>Load weights from a <code>.safetensors</code> file with named-axis remapping.</p> Source code in <code>src/fuse/interop.py</code> <pre><code>def from_safetensors(\n    path: Union[str, Path],\n    mapping: Mapping[str, Mapping[str, Any]],\n    *,\n    strict: bool = True,\n) -&gt; Dict[str, np.ndarray]:\n    \"\"\"\n    Load weights from a ``.safetensors`` file with named-axis remapping.\n    \"\"\"\n    try:\n        from safetensors.numpy import load_file as load_safetensors  # type: ignore\n    except Exception as exc:  # pragma: no cover - optional dependency\n        raise RuntimeError(\"safetensors is required for from_safetensors\") from exc\n\n    data = load_safetensors(str(path))\n    return from_pytorch(data, mapping, strict=strict)\n</code></pre>"},{"location":"reference/fuse/interop/#fuse.interop.to_torchscript","title":"<code>to_torchscript(program, example_inputs, *, policies=None, device='auto', config=None, file_path=None)</code>","text":"<p>Trace a Fuse program into a TorchScript module using example inputs.</p> Source code in <code>src/fuse/interop.py</code> <pre><code>def to_torchscript(\n    program: Program,\n    example_inputs: Mapping[str, Any],\n    *,\n    policies: Optional[RuntimePolicies] = None,\n    device: str = \"auto\",\n    config: Optional[Any] = None,\n    file_path: Optional[Union[str, Path]] = None,\n) -&gt; \"torch.jit.ScriptModule\":\n    \"\"\"\n    Trace a Fuse program into a TorchScript module using example inputs.\n    \"\"\"\n    _ensure_torch_available()\n    runtime_policies = _ensure_runtime_policies(policies)\n    runner = program.compile(\n        backend=\"torch\",\n        device=device,\n        config=config,\n        policies=runtime_policies,\n    )\n    input_names, tensors = _prepare_example_tensors(example_inputs, runner.device)\n    module = _FuseTorchModule(runner, input_names)\n    module.eval()\n    with torch.no_grad():\n        scripted = torch.jit.trace(module, tuple(tensors), strict=False)\n    if file_path is not None:\n        scripted.save(str(file_path))\n    return scripted\n</code></pre>"},{"location":"reference/fuse/interop/#fuse.interop.to_onnx","title":"<code>to_onnx(program, example_inputs, *, policies=None, device='auto', config=None, file_path=None, opset_version=17, dynamic_axes=None)</code>","text":"<p>Export a Fuse program to ONNX using PyTorch tracing.</p> Source code in <code>src/fuse/interop.py</code> <pre><code>def to_onnx(\n    program: Program,\n    example_inputs: Mapping[str, Any],\n    *,\n    policies: Optional[RuntimePolicies] = None,\n    device: str = \"auto\",\n    config: Optional[Any] = None,\n    file_path: Optional[Union[str, Path]] = None,\n    opset_version: int = 17,\n    dynamic_axes: Optional[Dict[str, Dict[int, str]]] = None,\n) -&gt; Union[bytes, Path]:\n    \"\"\"\n    Export a Fuse program to ONNX using PyTorch tracing.\n    \"\"\"\n    _ensure_torch_available()\n    runtime_policies = _ensure_runtime_policies(policies)\n    runner = program.compile(\n        backend=\"torch\",\n        device=device,\n        config=config,\n        policies=runtime_policies,\n    )\n    input_names, tensors = _prepare_example_tensors(example_inputs, runner.device)\n    module = _FuseTorchModule(runner, input_names)\n    module.eval()\n\n    output_names = list(program.ir.exports)\n\n    kwargs = {\n        \"input_names\": input_names,\n        \"output_names\": output_names,\n        \"opset_version\": opset_version,\n        \"dynamic_axes\": dynamic_axes or {},\n    }\n\n    if file_path is None:\n        buffer = io.BytesIO()\n        torch.onnx.export(\n            module,\n            tuple(tensors),\n            buffer,\n            **kwargs,\n        )\n        buffer.seek(0)\n        return buffer.getvalue()\n\n    torch.onnx.export(\n        module,\n        tuple(tensors),\n        str(file_path),\n        **kwargs,\n    )\n    return Path(file_path)\n</code></pre>"},{"location":"reference/fuse/jax_backend/","title":"fuse.jax_backend","text":""},{"location":"reference/fuse/jax_backend/#fuse.jax_backend","title":"<code>fuse.jax_backend</code>","text":"<p>JAX backend integration.</p>"},{"location":"reference/fuse/jax_backend/compile/","title":"fuse.jax_backend.compile","text":""},{"location":"reference/fuse/jax_backend/compile/#fuse.jax_backend.compile","title":"<code>fuse.jax_backend.compile</code>","text":""},{"location":"reference/fuse/logic/","title":"fuse.logic","text":""},{"location":"reference/fuse/logic/#fuse.logic","title":"<code>fuse.logic</code>","text":"<p>Logic program templates for symbolic and neuro-symbolic workloads.</p> <p>These helpers expose ready-to-parse :class:<code>~fuse.core.program.Program</code> instances so downstream users can assemble reasoning pipelines without rewriting recurring equation sets.</p>"},{"location":"reference/fuse/logic/#fuse.logic.aunt_analogy","title":"<code>aunt_analogy()</code>","text":"<p>Build a neuro-symbolic aunt inference template.</p> <p>Supply boolean relation tensors for <code>SisterFacts</code> and <code>ParentFacts</code>, along with dense <code>ObjectEmbeddings</code> aligned on the same entity axis. The program exports both the inferred <code>Aunt</code> relation and an analogical score tensor <code>Analogical[a,b]</code> suitable for ranking candidate pairs.</p> Source code in <code>src/fuse/logic/__init__.py</code> <pre><code>def aunt_analogy() -&gt; Program:\n    \"\"\"\n    Build a neuro-symbolic aunt inference template.\n\n    Supply boolean relation tensors for ``SisterFacts`` and ``ParentFacts``,\n    along with dense ``ObjectEmbeddings`` aligned on the same entity axis.\n    The program exports both the inferred ``Aunt`` relation and an analogical\n    score tensor ``Analogical[a,b]`` suitable for ranking candidate pairs.\n    \"\"\"\n\n    return Program(_AUNT_ANALOGY_EQUATIONS)\n</code></pre>"},{"location":"reference/fuse/nn/","title":"fuse.nn","text":""},{"location":"reference/fuse/nn/#fuse.nn","title":"<code>fuse.nn</code>","text":"<p>Neural network oriented program templates.</p> <p>This module provides ready-to-use :class:<code>~fuse.core.program.Program</code> factories implementing common building blocks such as attention layers, transformer encoders, and graph neural network message passing. Each factory returns a parsed <code>Program</code> whose sources are expected to be materialised via the runtime weight store (for example using :class:<code>~fuse.core.policies.InMemoryWeightStore</code>).</p>"},{"location":"reference/fuse/nn/#fuse.nn.attention_block","title":"<code>attention_block()</code>","text":"<p>Build a multi-head attention program.</p> <p>The program expects the following tensors to be resolvable via the weight store (names match :data:<code>ATTENTION_SOURCES</code>):</p> <p><code>InputStream[b,p,d]</code> \u2013 source activations <code>PositionalEncoding[b,p,d]</code> \u2013 additive positional signal (set to zeros when unused) <code>QueryWeight[h,dk,d]</code>, <code>KeyWeight[h,dk,d]</code>, <code>ValueWeight[h,dv,d]</code> \u2013 head projections <code>AttentionScale</code> \u2013 scalar (typically <code>1/sqrt(dk)</code>) <code>AttentionMask[b,h,p,p']</code> \u2013 additive mask per head; supply zeros to disable masking <code>OutputProj[d,h,dv]</code> \u2013 projection for concatenated head outputs</p>"},{"location":"reference/fuse/nn/#fuse.nn.attention_block--returns","title":"Returns","text":"<p>Program     Ready-to-run attention block exporting <code>Context</code>.</p> Source code in <code>src/fuse/nn/__init__.py</code> <pre><code>def attention_block() -&gt; Program:\n    \"\"\"\n    Build a multi-head attention program.\n\n    The program expects the following tensors to be resolvable via the weight\n    store (names match :data:`ATTENTION_SOURCES`):\n\n    ``InputStream[b,p,d]`` \u2013 source activations\n    ``PositionalEncoding[b,p,d]`` \u2013 additive positional signal (set to zeros when unused)\n    ``QueryWeight[h,dk,d]``, ``KeyWeight[h,dk,d]``, ``ValueWeight[h,dv,d]`` \u2013 head projections\n    ``AttentionScale`` \u2013 scalar (typically ``1/sqrt(dk)``)\n    ``AttentionMask[b,h,p,p']`` \u2013 additive mask per head; supply zeros to disable masking\n    ``OutputProj[d,h,dv]`` \u2013 projection for concatenated head outputs\n\n    Returns\n    -------\n    Program\n        Ready-to-run attention block exporting ``Context``.\n    \"\"\"\n\n    return Program(_ATTENTION_EQUATIONS)\n</code></pre>"},{"location":"reference/fuse/nn/#fuse.nn.transformer_encoder","title":"<code>transformer_encoder()</code>","text":"<p>Build a single transformer encoder layer (self-attention + MLP).</p> <p>Required tensor names are listed in :data:<code>TRANSFORMER_SOURCES</code>. Provide an attention scale scalar and a per-head additive mask compatible with the batch/sequence layout. The program exports both the post-MLP stream (<code>StreamOut</code>) and the token logits (<code>Y</code>).</p> Source code in <code>src/fuse/nn/__init__.py</code> <pre><code>def transformer_encoder() -&gt; Program:\n    \"\"\"\n    Build a single transformer encoder layer (self-attention + MLP).\n\n    Required tensor names are listed in :data:`TRANSFORMER_SOURCES`. Provide an\n    attention scale scalar and a per-head additive mask compatible with the\n    batch/sequence layout. The program exports both the post-MLP stream\n    (``StreamOut``) and the token logits (``Y``).\n    \"\"\"\n\n    return Program(_TRANSFORMER_EQUATIONS)\n</code></pre>"},{"location":"reference/fuse/nn/#fuse.nn.graph_message_passing","title":"<code>graph_message_passing(num_layers=2)</code>","text":"<p>Build a stacked message passing GNN with linear self/neighbor mixing.</p>"},{"location":"reference/fuse/nn/#fuse.nn.graph_message_passing--parameters","title":"Parameters","text":"<p>num_layers:     How many message passing iterations to include. Each layer expects     per-layer weights <code>MessageWeight{l}</code>, <code>AggWeight{l}</code>, and     <code>SelfWeight{l}</code> with square <code>[d,d]</code> layout, plus a pooling vector     <code>GraphPooling[n]</code> to aggregate node embeddings.</p>"},{"location":"reference/fuse/nn/#fuse.nn.graph_message_passing--returns","title":"Returns","text":"<p>Program     Program exporting <code>Embeddings</code>, node probabilities, edge logits, and     graph-level probabilities.</p> Source code in <code>src/fuse/nn/__init__.py</code> <pre><code>def graph_message_passing(num_layers: int = 2) -&gt; Program:\n    \"\"\"\n    Build a stacked message passing GNN with linear self/neighbor mixing.\n\n    Parameters\n    ----------\n    num_layers:\n        How many message passing iterations to include. Each layer expects\n        per-layer weights ``MessageWeight{l}``, ``AggWeight{l}``, and\n        ``SelfWeight{l}`` with square ``[d,d]`` layout, plus a pooling vector\n        ``GraphPooling[n]`` to aggregate node embeddings.\n\n    Returns\n    -------\n    Program\n        Program exporting ``Embeddings``, node probabilities, edge logits, and\n        graph-level probabilities.\n    \"\"\"\n\n    spec = _GNNSpec(num_layers=num_layers)\n    return Program(spec.build_equations())\n</code></pre>"},{"location":"reference/fuse/nn/#fuse.nn.graph_message_passing_sources","title":"<code>graph_message_passing_sources(num_layers=2)</code>","text":"<p>List the tensor names the :func:<code>graph_message_passing</code> program expects.</p> Source code in <code>src/fuse/nn/__init__.py</code> <pre><code>def graph_message_passing_sources(num_layers: int = 2) -&gt; Tuple[str, ...]:\n    \"\"\"\n    List the tensor names the :func:`graph_message_passing` program expects.\n    \"\"\"\n\n    spec = _GNNSpec(num_layers=num_layers)\n    return spec.required_sources()\n</code></pre>"},{"location":"reference/fuse/package/","title":"fuse.package","text":""},{"location":"reference/fuse/package/#fuse.package","title":"<code>fuse.package</code>","text":""},{"location":"reference/fuse/pgm/","title":"fuse.pgm","text":""},{"location":"reference/fuse/pgm/#fuse.pgm","title":"<code>fuse.pgm</code>","text":"<p>Probabilistic graphical model program templates.</p> <p>The helpers here provide junction-tree friendly equation sets for factor graphs and small hidden Markov models that can be compiled directly with the Fuse runtime.</p>"},{"location":"reference/fuse/pgm/#fuse.pgm.factor_graph_triplet","title":"<code>factor_graph_triplet()</code>","text":"<p>Build a simple pairwise factor graph over three variables.</p> <p>Provide unary and pairwise potentials matching :data:<code>FACTOR_GRAPH_SOURCES</code>. The program exports the joint tensor and the normalized unary marginals.</p> Source code in <code>src/fuse/pgm/__init__.py</code> <pre><code>def factor_graph_triplet() -&gt; Program:\n    \"\"\"\n    Build a simple pairwise factor graph over three variables.\n\n    Provide unary and pairwise potentials matching\n    :data:`FACTOR_GRAPH_SOURCES`. The program exports the joint tensor and the\n    normalized unary marginals.\n    \"\"\"\n\n    return Program(_FACTOR_GRAPH_EQUATIONS)\n</code></pre>"},{"location":"reference/fuse/pgm/#fuse.pgm.hmm_forward_two_step","title":"<code>hmm_forward_two_step()</code>","text":"<p>Build a two-observation Hidden Markov Model forward recursion.</p> <p>The observation selector constants can be replaced or extended downstream by modifying the returned program source if more timesteps are required.</p> Source code in <code>src/fuse/pgm/__init__.py</code> <pre><code>def hmm_forward_two_step() -&gt; Program:\n    \"\"\"\n    Build a two-observation Hidden Markov Model forward recursion.\n\n    The observation selector constants can be replaced or extended downstream\n    by modifying the returned program source if more timesteps are required.\n    \"\"\"\n\n    return Program(_HMM_FORWARD_EQUATIONS)\n</code></pre>"},{"location":"reference/fuse/torch_backend/","title":"fuse.torch_backend","text":""},{"location":"reference/fuse/torch_backend/#fuse.torch_backend","title":"<code>fuse.torch_backend</code>","text":"<p>PyTorch backend integration.</p>"},{"location":"reference/fuse/torch_backend/compile/","title":"fuse.torch_backend.compile","text":""},{"location":"reference/fuse/torch_backend/compile/#fuse.torch_backend.compile","title":"<code>fuse.torch_backend.compile</code>","text":""},{"location":"reference/fuse/training/","title":"fuse.training","text":""},{"location":"reference/fuse/training/#fuse.training","title":"<code>fuse.training</code>","text":""},{"location":"reference/fuse/training/#fuse.training.gradients_for_program","title":"<code>gradients_for_program(program, *, seeds, grad_tensors, backend='numpy', config=None, policies=None)</code>","text":"<p>Evaluate per-example gradients for the provided program by generating a symbolic gradient program (BPTS-style).</p>"},{"location":"reference/fuse/training/#fuse.training.gradients_for_program--parameters","title":"Parameters","text":"<p>program:     Compiled representation of the example-specific Fuse program. seeds:     Mapping of tensor name to Fuse expression used to seed its gradient     (e.g., <code>{\"Loss\": \"const(1.0)\"}</code>). grad_tensors:     Iterable of tensor names whose gradients should be returned. backend:     Backend used to execute the gradient program (defaults to \"numpy\"). config:     Optional execution config; defaults to <code>ExecutionConfig(mode=\"single\")</code>. policies:     Optional runtime policies passed to the compiled gradient program.</p>"},{"location":"reference/fuse/training/#fuse.training.gradients_for_program--returns","title":"Returns","text":"<p>grads:     Dictionary mapping tensor names to <code>np.ndarray</code> gradients. raw_outputs:     Complete output dictionary from the gradient program execution.</p> Source code in <code>src/fuse/training/bpts.py</code> <pre><code>def gradients_for_program(\n    program: Program,\n    *,\n    seeds: Dict[str, str],\n    grad_tensors: Sequence[str],\n    backend: str = \"numpy\",\n    config: Optional[ExecutionConfig] = None,\n    policies: Optional[RuntimePolicies] = None,\n) -&gt; Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n    \"\"\"\n    Evaluate per-example gradients for the provided program by generating\n    a symbolic gradient program (BPTS-style).\n\n    Parameters\n    ----------\n    program:\n        Compiled representation of the example-specific Fuse program.\n    seeds:\n        Mapping of tensor name to Fuse expression used to seed its gradient\n        (e.g., ``{\\\"Loss\\\": \\\"const(1.0)\\\"}``).\n    grad_tensors:\n        Iterable of tensor names whose gradients should be returned.\n    backend:\n        Backend used to execute the gradient program (defaults to \"numpy\").\n    config:\n        Optional execution config; defaults to ``ExecutionConfig(mode=\\\"single\\\")``.\n    policies:\n        Optional runtime policies passed to the compiled gradient program.\n\n    Returns\n    -------\n    grads:\n        Dictionary mapping tensor names to ``np.ndarray`` gradients.\n    raw_outputs:\n        Complete output dictionary from the gradient program execution.\n    \"\"\"\n    grad_program = generate_gradient_program(\n        program,\n        seeds=seeds,\n        export_grads=grad_tensors,\n    )\n    runner = grad_program.program.compile(\n        backend=backend,\n        config=config or ExecutionConfig(mode=\"single\"),\n        policies=policies,\n    )\n    outputs = runner()\n    grads: Dict[str, np.ndarray] = {}\n    for tensor in grad_tensors:\n        name = _grad_name(tensor)\n        grad_value = outputs.get(name)\n        if grad_value is None:\n            raise KeyError(f\"Gradient output '{name}' missing from gradient program\")\n        grads[tensor] = np.asarray(grad_value)\n    return grads, outputs\n</code></pre>"},{"location":"reference/fuse/training/bpts/","title":"fuse.training.bpts","text":""},{"location":"reference/fuse/training/bpts/#fuse.training.bpts","title":"<code>fuse.training.bpts</code>","text":""},{"location":"reference/fuse/training/bpts/#fuse.training.bpts.gradients_for_program","title":"<code>gradients_for_program(program, *, seeds, grad_tensors, backend='numpy', config=None, policies=None)</code>","text":"<p>Evaluate per-example gradients for the provided program by generating a symbolic gradient program (BPTS-style).</p>"},{"location":"reference/fuse/training/bpts/#fuse.training.bpts.gradients_for_program--parameters","title":"Parameters","text":"<p>program:     Compiled representation of the example-specific Fuse program. seeds:     Mapping of tensor name to Fuse expression used to seed its gradient     (e.g., <code>{\"Loss\": \"const(1.0)\"}</code>). grad_tensors:     Iterable of tensor names whose gradients should be returned. backend:     Backend used to execute the gradient program (defaults to \"numpy\"). config:     Optional execution config; defaults to <code>ExecutionConfig(mode=\"single\")</code>. policies:     Optional runtime policies passed to the compiled gradient program.</p>"},{"location":"reference/fuse/training/bpts/#fuse.training.bpts.gradients_for_program--returns","title":"Returns","text":"<p>grads:     Dictionary mapping tensor names to <code>np.ndarray</code> gradients. raw_outputs:     Complete output dictionary from the gradient program execution.</p> Source code in <code>src/fuse/training/bpts.py</code> <pre><code>def gradients_for_program(\n    program: Program,\n    *,\n    seeds: Dict[str, str],\n    grad_tensors: Sequence[str],\n    backend: str = \"numpy\",\n    config: Optional[ExecutionConfig] = None,\n    policies: Optional[RuntimePolicies] = None,\n) -&gt; Tuple[Dict[str, np.ndarray], Dict[str, np.ndarray]]:\n    \"\"\"\n    Evaluate per-example gradients for the provided program by generating\n    a symbolic gradient program (BPTS-style).\n\n    Parameters\n    ----------\n    program:\n        Compiled representation of the example-specific Fuse program.\n    seeds:\n        Mapping of tensor name to Fuse expression used to seed its gradient\n        (e.g., ``{\\\"Loss\\\": \\\"const(1.0)\\\"}``).\n    grad_tensors:\n        Iterable of tensor names whose gradients should be returned.\n    backend:\n        Backend used to execute the gradient program (defaults to \"numpy\").\n    config:\n        Optional execution config; defaults to ``ExecutionConfig(mode=\\\"single\\\")``.\n    policies:\n        Optional runtime policies passed to the compiled gradient program.\n\n    Returns\n    -------\n    grads:\n        Dictionary mapping tensor names to ``np.ndarray`` gradients.\n    raw_outputs:\n        Complete output dictionary from the gradient program execution.\n    \"\"\"\n    grad_program = generate_gradient_program(\n        program,\n        seeds=seeds,\n        export_grads=grad_tensors,\n    )\n    runner = grad_program.program.compile(\n        backend=backend,\n        config=config or ExecutionConfig(mode=\"single\"),\n        policies=policies,\n    )\n    outputs = runner()\n    grads: Dict[str, np.ndarray] = {}\n    for tensor in grad_tensors:\n        name = _grad_name(tensor)\n        grad_value = outputs.get(name)\n        if grad_value is None:\n            raise KeyError(f\"Gradient output '{name}' missing from gradient program\")\n        grads[tensor] = np.asarray(grad_value)\n    return grads, outputs\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>This section walks through running the example programs under <code>examples/</code>.</p>"},{"location":"tutorials/#run-the-gallery-numpy-backend","title":"Run the gallery (NumPy backend)","text":"<pre><code>python examples/run_new_architectures.py --backend numpy\n</code></pre> <p>Artifacts are written under <code>examples/runs/</code>.</p>"},{"location":"tutorials/#run-a-single-program","title":"Run a single program","text":"<pre><code>python -m fuse run examples/04_mlp.fuse --backend numpy\n</code></pre> <p>Use <code>--backend torch</code> or <code>--backend jax</code> if the optional backends are installed.</p>"}]}